{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Assessment 2\n",
    "\n",
    "Through the following notebook, you will be tasked with **implementing** two different classification approaches: **k Nearest Neighbours** and **Decision Trees**. You will apply these to two different datasets, evaluate the produced models and analyse their performance.\n",
    "\n",
    "This will be done through several different tasks and sub-tasks:\n",
    "- [Dataset](#Dataset)\n",
    "    - [Task 1: Dataset statistics **(10\\%)**](#Task-1---Dataset-statistics)\n",
    "- [k Nearest Neigbours](#k-Nearest-Neigbours)\n",
    "    - [Task 2.1: Euclidean distance **(10\\%)**](#Task-2.1---Euclidean-distance)\n",
    "    - [Task 2.2: kNN classifier **(20\\%)**](#Task-2.2---kNN-classifier)\n",
    "- [Decision Trees](#Decision-Trees)\n",
    "    - [Task 3.1: Entropy and Information Gain **(10\\%)**](#Task-3.1---Entropy-and-Information-Gain)\n",
    "    - [Task 3.2: Decision Tree classifier **(30\\%)**](#Task-3.2---Decision-Tree-classifier)\n",
    "- [Model evaluation and analysis](#Model-evaluation-and-analysis)\n",
    "    - [Task 4.1: Evaluation **(10\\%)**](#Task-4.1---Evaluation)\n",
    "    - [Task 4.2: Analysis **(10\\%)**](#Task-4.2---Analysis)\n",
    "\n",
    "**Note:** The (%) noted above are out of 100; this will be scaled down to **maximum of 70 marks** for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this assessment, you will be working with two datasets:\n",
    "- A [numeric-only](#Numerical-data) dataset (crystal systems of Li-ion batteries)\n",
    "- A dataset with [mixed numeric and categorical](#Categorical-data) features (forest cover)\n",
    "\n",
    "For developing your solutions, **you have only been provided with a portion of the samples from each of the dataset** (70% of the [numeric-only](#Numerical-data) and 50% of the [mixed numeric and categorical](#Categorical-data) data). These will be in **identical format** to the full datasets used for evaluating your solutions (there will just be more samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical data\n",
    "\n",
    "The main part of the assessment will be done on the dataset containing only numerical features describing the physical and chemical properties of the Li-ion battery, which can be classified on the basis of their crystal system. Three major classes of crystal systems are: _monoclinic_, _orthorhombic_ and _triclinic_. (The dataset for this assessment has been adapted from the full dataset which can be found [here](https://www.kaggle.com/datasets/divyansh22/crystal-system-properties-for-liion-batteries\n",
    "\n",
    "Each sample corresponds to the properties of a battery, and consists of following features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Formation Energy`       | `float`: eV | Formation energy of the material. |\n",
    "| `E Above Hull` | `float`: eV | Energy of decomposition of material into most stable ones. |\n",
    "| `Band Gap` | `float`: eV | Band gap. |\n",
    "| `Nsites` | `int`: count | Number of atoms in the unit cell of the crystal. |\n",
    "| `Density` | `float`: gm/cc | The density of bulk crystalline materials. |\n",
    "| `Volume` | `float` | The unit cell volume of the material. |\n",
    "\n",
    "The goal for the assessment is to predict whether the crystal system of the battery is _monoclinic_, _orthorhombic_ or _triclinic_, which provides a classification for each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Crystal System`  | `string`: class designation | Class of the crystal system (three different values). |\n",
    "\n",
    "\n",
    "This dataset will be used for the developmet and testing of both the [kNN](#k-Nearest-Neigbours) and [Decision Tree](#Decision-Trees) classifier, as well as for the [evaluation](#Task-4.1---Evaluation) of both of the classification approaches.\n",
    "\n",
    "The following cell loads the dataset, and splits the samples randomly, using  $80\\%$ of samples for training and $20\\%$ for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 237 samples\n",
      "The numerical data loaded consists of following columns:\n",
      "\tColumn 0: Formation Energy\n",
      "\tColumn 1: E Above Hull\n",
      "\tColumn 2: Band Gap\n",
      "\tColumn 3: Nsites\n",
      "\tColumn 4: Density\n",
      "\tColumn 5: Volume\n",
      "\tColumn 6: Crystal System\n",
      "An example sample:\n",
      "\tFeature 0: Formation Energy=-2.555\n",
      "\tFeature 1: E Above Hull=0.069\n",
      "\tFeature 2: Band Gap=1.854\n",
      "\tFeature 3: Nsites=15\n",
      "\tFeature 4: Density=2.925\n",
      "\tFeature 5: Volume=179.798\n",
      "\tClass (Family): triclinic\n",
      "The training set consists of 189 samples and 189 associated ground truth classes\n",
      "The test set consists of 48 samples and 48 associated ground truth classes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import count\n",
    "\n",
    "numerical_data = pd.read_csv(r'batteries.csv')\n",
    "\n",
    "print(\"The dataset consists of {} samples\".format(len(numerical_data)))\n",
    "print(\"The numerical data loaded consists of following columns:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tColumn {}: {}\".format(i, x) for i, x in enumerate(numerical_data.columns)])))\n",
    "print(\"An example sample:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tFeature {}: {}={}\".format(i, x, v) for i, (x, v) in \n",
    "               enumerate(zip(numerical_data.columns, numerical_data.loc[0, :])) if not x == 'Crystal System'])))\n",
    "print('\\tClass (Family): {}'.format(numerical_data.loc[0, 'Crystal System']))\n",
    "\n",
    "# We split the dataset into features and the target class, using the 'Family' column as the target:\n",
    "X_nd = numerical_data.loc[:, numerical_data.columns!='Crystal System'].values\n",
    "y_nd = numerical_data['Crystal System'].values\n",
    "\n",
    "# We further split the dataset into a training and testing set\n",
    "X_nd_train, X_nd_test, y_nd_train, y_nd_test = train_test_split(X_nd, y_nd, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"The training set consists of {} samples and {} associated ground truth classes\".format(len(X_nd_train),\n",
    "                                                                                             len(y_nd_train)))\n",
    "print(\"The test set consists of {} samples and {} associated ground truth classes\".format(len(X_nd_test),\n",
    "                                                                                             len(y_nd_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data\n",
    "\n",
    "The second dataset for this assessment contains a mix of numerical and categorical features, with the goal of predicting the forest cover type. The data used in this assessment is adapted from a full dataset provided [here](https://archive.ics.uci.edu/ml/datasets/Covertype) (Jock A. Blackard and Colorado State University).\n",
    "\n",
    "Each sample corresponds to a 30x30 meter cell, and consists of cartographic variables only (i.e. it does not rely on any remotely sensed imaging data), derived from from US Geological Survey (USGS) and USFS data. Each sample consists of the following features (categorical features can be distinguished by being encoded as a string rather tha a numer):\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Elevation`       | `float`: kilometers | Elevation |\n",
    "| `Aspect`   | `int`: degrees azimut | Aspect in degreez azimuth |\n",
    "| `Slope`      | `int`: degrees | Slope in degrees |\n",
    "| `Horizontal_Distance_To_Hydrology`   | `float`: kilometers | Horiz. distance to nearest surface water feature        |\n",
    "| `Vertical_Distance_To_Hydrology`   | `float`: kilometers | Vert. distance to nearest surface water feature        |\n",
    "| `Horizontal_Distance_To_Roadways`     | `float`: kilometers | Hor. distance to nearest roadway       |\n",
    "| `Hillshade_9am`   |`int`: $0 \\dots 255 $|  Hillshade index at 9am, summer solstice        |\n",
    "| `Hillshade_Noon`      |`int`: $0 \\dots 255 $| Hillshade index at noon, summer solstice       |\n",
    "| `Hillshade_3pm`   |`int`: $0 \\dots 255 $| Hillshade index at 3pm, summer solstice        |\n",
    "| `Horizontal_Distance_To_Fire_Points`  | `float`: kilometers | Horiz. distance to nearest wildfire ignition point        |\n",
    "| `Wilderness_Area`   | `string`: name | Wilderness area name (4 different values)       |\n",
    "| `Soil_Type`   |`string`: code| Soil type code (40 different values)       |\n",
    "\n",
    "The goal of this dataset is to predict the forrest cover type for each 30x30 meter cell. A class, corresponding to the forest cover type, is associated to each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Cover_Type`       | `string`: name | Forest cover type designation (7 different values). |\n",
    "\n",
    "\n",
    "**Note:** Tackling the categorical dataset is a **stretch task** for this assessment. It will be used in the development and testing of the [Decision Tree](#Decision-Trees) classifier, as well as for their [evaluation](#Task-4.1---Evaluation). Tackling this data will **require you to implement a** `DecisionTree` **from scratch**, as `sklearn` **does not provide this functionality**.\n",
    "\n",
    "The following cell loads the dataset, and splits the samples randomly, using  $80\\%$ of samples for training and $20\\%$ for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 58101 samples\n",
      "The numerical data loaded consists of following columns:\n",
      "\tColumn 0: Elevation\n",
      "\tColumn 1: Aspect\n",
      "\tColumn 2: Slope\n",
      "\tColumn 3: Horizontal_Distance_To_Hydrology\n",
      "\tColumn 4: Vertical_Distance_To_Hydrology\n",
      "\tColumn 5: Horizontal_Distance_To_Roadways\n",
      "\tColumn 6: Hillshade_9am\n",
      "\tColumn 7: Hillshade_Noon\n",
      "\tColumn 8: Hillshade_3pm\n",
      "\tColumn 9: Horizontal_Distance_To_Fire_Points\n",
      "\tColumn 10: Wilderness_Area\n",
      "\tColumn 11: Soil_Type\n",
      "\tColumn 12: Cover_Type\n",
      "An example sample:\n",
      "\tFeature 0: Elevation=3.0\n",
      "\tFeature 1: Aspect=87.0\n",
      "\tFeature 2: Slope=9.0\n",
      "\tFeature 3: Horizontal_Distance_To_Hydrology=0.3\n",
      "\tFeature 4: Vertical_Distance_To_Hydrology=0.0\n",
      "\tFeature 5: Horizontal_Distance_To_Roadways=3.7\n",
      "\tFeature 6: Hillshade_9am=233.0\n",
      "\tFeature 7: Hillshade_Noon=226.0\n",
      "\tFeature 8: Hillshade_3pm=125.0\n",
      "\tFeature 9: Horizontal_Distance_To_Fire_Points=2.2\n",
      "\tFeature 10: Wilderness_Area=Rawah\n",
      "\tFeature 11: Soil_Type=ELU-7745\n",
      "\tClass (Cover_Type): Lodgepole_Pine\n",
      "The training set consists of 46480 samples and 46480 associated ground truth classes\n",
      "The test set consists of 11621 samples and 11621 associated ground truth classes\n"
     ]
    }
   ],
   "source": [
    "categorical_data = pd.read_csv(r'forestcover.csv')\n",
    "\n",
    "print(\"The dataset consists of {} samples\".format(len(categorical_data)))\n",
    "print(\"The numerical data loaded consists of following columns:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tColumn {}: {}\".format(i, x) for i, x in enumerate(categorical_data.columns)])))\n",
    "print(\"An example sample:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tFeature {}: {}={}\".format(i, x, v) for i, (x, v) in \n",
    "               enumerate(zip(categorical_data.columns, categorical_data.loc[0, :])) if not x == 'Cover_Type'])))\n",
    "print('\\tClass (Cover_Type): {}'.format(categorical_data.loc[0, 'Cover_Type']))\n",
    "\n",
    "# We split the dataset into features and the target class, using the 'Cover_Type' column as the target:\n",
    "X_cd = categorical_data.loc[:, categorical_data.columns!='Cover_Type'].values\n",
    "y_cd = categorical_data['Cover_Type'].values\n",
    "\n",
    "\n",
    "# We further split the dataset into a training and testing set\n",
    "X_cd_train, X_cd_test, y_cd_train, y_cd_test = train_test_split(X_cd, y_cd, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"The training set consists of {} samples and {} associated ground truth classes\".format(len(X_cd_train),\n",
    "                                                                                             len(y_cd_train)))\n",
    "print(\"The test set consists of {} samples and {} associated ground truth classes\".format(len(X_cd_test),\n",
    "                                                                                             len(y_cd_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Dataset statistics\n",
    "#### 10% marks\n",
    "\n",
    "To handle the data for the classification tasks, implement functions that will assist in querying some basic information about the dataset you are handling.\n",
    "\n",
    "Given a set of M samples $\\mathbf{x} = \\{\\mathbf{x}_i\\} \\forall i=1..M$ with n features each $\\mathbf{x}_i = \\{x_{1,i}, \\dots, x_{n,i}\\}$, and the corresponding ground truth labels $\\mathbf{y} = \\{y_i\\} \\forall i=1..M$, implement two functions:\n",
    "\n",
    "- `class_probabilities(y)` : calcualtes the class probabilities from the list of ground truth labels\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `y`: The list of ground truth labels for a dataset<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    A dictionary of class labels and their probabilities (i.e. the frequency of that class in the dataset)\n",
    "\n",
    "    *Example* :\n",
    "    ```python\n",
    "    y = [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
    "    print(class_probabilities(y))\n",
    "    ```\n",
    "    *Example output*:<br>\n",
    "    `{0: 0.3, 1: 0.5, 2: 0.2}`\n",
    "    \n",
    "    *Note:* Python dictionaries are unordered, so running this example might result in a different order of keys in the output, for example: `{2: 0.2, 0: 0.3, 1: 0.5}`<br><br>\n",
    "   \n",
    "- `most_common_class` : finds the most common class from the list of ground truth class labels.\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `y`: the list of ground truth labels for a dataset<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    the label of the most common class represented in the dataset.\n",
    "    \n",
    "    *Example*:\n",
    "    ```python\n",
    "    y = [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
    "    print(most_common_class(y))\n",
    "    ```\n",
    "    *Example output*:<br>\n",
    "    `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(y): \n",
    "    classDict = collections.defaultdict(float) #establishes an empty dictionary that will only take floats \n",
    "    for i in range(len(y)): \n",
    "        if y[i] in classDict: \n",
    "            classDict[y[i]] += 1.0 #increases the value of the key (which is the class) \n",
    "        else: \n",
    "            classDict[y[i]] = 1.0 #makes a new key for a newly found class \n",
    "    totalClasses = sum(classDict.values()) #sum together the values \n",
    "    for key in classDict: \n",
    "        classDict[key] /= totalClasses #gets the probability of each class which is number of instances/total number\n",
    "    return classDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_class(y): \n",
    "    classDict = collections.defaultdict(int) #establishes an empty dictionary that will only take ints \n",
    "    for i in range(len(y)): \n",
    "        if y[i] in classDict: \n",
    "            classDict[y[i]] += 1 #increases the value of the key (which is the class)\n",
    "        else: \n",
    "            classDict[y[i]] = 1 #makes a new key for a newly found class \n",
    "    sortedClass = sorted(classDict.items(), key = lambda x:x[1], reverse = True) #sorts in ascending order the dictionary\n",
    "    return sortedClass[0][0] #returns the value seen the most (mode). If there are two modes, returns the first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique class labels and counts from numerical-only dataset: monoclinic: 92 orthorhombic: 89 triclinic: 56\n",
      "All classes and their probabilities:\n",
      "\ttriclinic: 0.23628691983122363\n",
      "\tmonoclinic: 0.3881856540084388\n",
      "\torthorhombic: 0.3755274261603376\n",
      "Most common class in the numerical-only dataset: monoclinic\n",
      "\n",
      "\n",
      "All unique class labels for the numerical-only training set: monoclinic: 76 orthorhombic: 69 triclinic: 44\n",
      "All classes and their probabilities (num-only training set):\n",
      "\ttriclinic: 0.2328042328042328\n",
      "\tmonoclinic: 0.4021164021164021\n",
      "\torthorhombic: 0.36507936507936506\n",
      "Most common class in the num-only training set: monoclinic\n",
      "\n",
      "\n",
      "All unique class labels for the numerical-only testing set: monoclinic: 16 orthorhombic: 20 triclinic: 12\n",
      "All classes and their probabilities (num-only tests set):\n",
      "\ttriclinic: 0.25\n",
      "\tmonoclinic: 0.3333333333333333\n",
      "\torthorhombic: 0.4166666666666667\n",
      "Most common class in the num-only test set: orthorhombic\n",
      "\n",
      "\n",
      "All class labels for a toy set: [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
      "All classses and their probabilities for the toy set:\n",
      "\t1: 0.5\n",
      "\t0: 0.3\n",
      "\t2: 0.2\n",
      "Most common class in the toy set: 1\n",
      "\n",
      "\n",
      "All classes and their probabilities from the categorical dataset:\n",
      "\tLodgepole_Pine: 0.486\n",
      "\tKrummholz: 0.035\n",
      "\tSpruce/Fir: 0.367\n",
      "\tPonderosa_Pine: 0.062\n",
      "\tDouglas-fir: 0.029\n",
      "\tAspen: 0.017\n",
      "\tCottonwood/Willow: 0.005\n",
      "Most common class in the categorical dataset: Lodgepole_Pine\n"
     ]
    }
   ],
   "source": [
    "unq, cnt = np.unique(y_nd, return_counts=True)\n",
    "print(\"All unique class labels and counts from numerical-only dataset: {}\".format(\n",
    "    ' '.join(['{}: {}'.format(v,c) for v, c in zip(unq, cnt)])))\n",
    "print(\"All classes and their probabilities:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_nd).items()])))\n",
    "print(\"Most common class in the numerical-only dataset: {}\".format(most_common_class(y_nd)))\n",
    "print(\"\\n\")\n",
    "\n",
    "unq, cnt = np.unique(y_nd_train, return_counts=True)\n",
    "print(\"All unique class labels for the numerical-only training set: {}\".format(\n",
    "    ' '.join(['{}: {}'.format(v,c) for v, c in zip(unq, cnt)])))\n",
    "print(\"All classes and their probabilities (num-only training set):\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_nd_train).items()])))\n",
    "print(\"Most common class in the num-only training set: {}\".format(most_common_class(y_nd_train)))\n",
    "print(\"\\n\")\n",
    "\n",
    "unq, cnt = np.unique(y_nd_test, return_counts=True)\n",
    "print(\"All unique class labels for the numerical-only testing set: {}\".format(\n",
    "    ' '.join(['{}: {}'.format(v,c) for v, c in zip(unq, cnt)])))\n",
    "print(\"All classes and their probabilities (num-only tests set):\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_nd_test).items()])))\n",
    "print(\"Most common class in the num-only test set: {}\".format(most_common_class(y_nd_test)))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test = [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
    "print(\"All class labels for a toy set: {}\".format(y_test))\n",
    "print(\"All classses and their probabilities for the toy set:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_test).items()])))\n",
    "print(\"Most common class in the toy set: {}\".format(most_common_class(y_test)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"All classes and their probabilities from the categorical dataset:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {:.3f}\".format(c, p) for c, p in class_probabilities(y_cd).items()])))\n",
    "print(\"Most common class in the categorical dataset: {}\".format(most_common_class(y_cd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbours\n",
    "\n",
    "For the first part of the assessment, you are required to implement **k Nearest Neigbours**. A key concept for kNN (and many different machine learning algorithms) is that of **distance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Euclidean distance\n",
    "#### 10% marks\n",
    "\n",
    "One of the most commonly used distance metrics is the Euclidean distance. The Euclidean distance between two points in the Euclidean space (also known as $L_2$ norm) is defined as the length of the line segment between those two points. For example, if we have two points in 2D space (i.e. two samples consisting of two features), $\\mathbf{x}_0 = \\{x_{0,0}, x_{0,1}\\}$ and $\\mathbf{x}_1 = \\{x_{1,0}, x_{1,1}\\}$, we can calculate the distance as:\n",
    "\n",
    "\\begin{equation*}\n",
    "d_{\\texttt{Euclidean}} = \\sqrt{(x_{0,0} - x_{1,0})^2 + (x_{0,1} - x_{1,1})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, we need to calculate the difference between the first features of the two samples, the difference between the second features of the two samples, then square each of the differences, sum those squares, and calculate the square root of the resulting sum.\n",
    "\n",
    "In general, when we are dealing with points in nD space (corresponding to samples with n features) of the form $\\mathbf{x}_i = \\{x_{0,i}, x_{1,i}, \\dots, x_{n,i}\\}$, the procedure is very similar. To calculade the Euclidean disatnce, you need to calculate the difference between the corresponding features of two samples, square each of the differences, and calculate the square root of the resulting sum:\n",
    "\n",
    "\\begin{equation*}\n",
    "d_{\\texttt{Euclidean}} = \\sqrt{\\sum_i{(x_{0,i} - x_{1,i})^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "Implement the Euclidean distance as a Python function:\n",
    "- `euclidean_distance`: calculates the Euclidean distance between two n-dimensional samples\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `x1`: An n-dimensional sample. This either a list of length $n$, or a `numpy.array` of dimensions `(n,)`<br>\n",
    "    `x2`: An n-dimensional sample. This either a list of length $n$, or a `numpy.array` of dimensions `(n,)`<br>\n",
    "\n",
    "    *Returns*<br>\n",
    "    The Euclidean distance between `x1` and `x2`.<br>\n",
    "    \n",
    "    *Example:*\n",
    "\n",
    "    ```python\n",
    "    x1 = [7, -1]\n",
    "    x2 = [4, 3]\n",
    "    print(euclidean_distance(x1, x2)\n",
    "    ```\n",
    "    *Example output:*<br>\n",
    "    `5.0`\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    differenceSquared = 0 #will have a sum of squared values \n",
    "    for i in range(len(x1)): #goes through all the values of the array as euclidian distance works through n dimensions  \n",
    "        differenceSquared += (x1[i] - x2[i]) ** 2 #works out the difference between the two values at dimension i and squares them\n",
    "    differenceSquared = math.sqrt(differenceSquared) #square roots the total for euclidian distance\n",
    "    return differenceSquared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between [7, -1] (2D) and [4, 3] (2D) is 5.0\n",
      "Euclidean distance between [1, 2] (2D) and [3, 4] (2D) is 2.828\n",
      "Euclidean distance between [4.2, 3.6, 1.0] (3D) and [5.7, 2.8, -1.5] (3D) is 3.023\n",
      "Euclidean distance between [0.20488259 0.60840624 0.86450847 0.81396427 0.26677942 0.28907039\n",
      " 0.5474658  0.51307861 0.31265578 0.60960883 0.7505621  0.68713959\n",
      " 0.82353012 0.58371389 0.95004835 0.15862421 0.94601071 0.59987245\n",
      " 0.82913879 0.51537348] (20D) and [0.19027926 0.9803218  0.05037757 0.44190665 0.71379359 0.65414937\n",
      " 0.27825911 0.164024   0.62912732 0.90166808 0.50827217 0.86584792\n",
      " 0.07605024 0.80767127 0.63098106 0.3830714  0.72020191 0.44455909\n",
      " 0.04767279 0.48849142] (20D) is 1.786\n"
     ]
    }
   ],
   "source": [
    "a = [7, -1]\n",
    "b = [4, 3]\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))\n",
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {:.3f}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))\n",
    "a = [4.2, 3.6, 1.0]\n",
    "b = [5.7, 2.8, -1.5]\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {:.3f}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))\n",
    "a = np.random.rand(20)\n",
    "b = np.random.rand(20)\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {:.3f}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - kNN classifier\n",
    "#### 20% marks\n",
    "\n",
    "K nearest neighbours classifier will predict the class of a sample based on the class label of its $k$ nearest neigbours, according to a given distance metric. Implement `kNN` classifier which works on samples with numerical features, and relies on the Euclidean distance, as a Python class implementing the following member functions:\n",
    "\n",
    "- `__init__(self, k = 5, distance = euclidean_distance)`: class constructor\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `k` : The number of neigbours to be considered by the classification model <br>\n",
    "    `distance`: Tunction which is used to calculate the distance between samples (you will use the Euclidean distance implemented in the previous task)<br><br>\n",
    "    \n",
    "- `fit(self, X, y)`: member function used to train the classifier (fit the model to the data)\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The dataset samples. This is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, n)`<br>\n",
    "    `y`: The dataset labels. This is either a list of length $M$, or a `numpy.array` of dimensions `(M,)`<br><br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    The trained classifier model.<br><br>\n",
    "    \n",
    "- `predict(self, X)`: member function used to predict the classes of samples `X`\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The samples for which to predict a class. If this is a single sample, then `X` is either a list of length $n$, or a `numpy.array` of dimensions `(n,)`. If this is a set of samples, then `X` is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, N)`<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    Predicted class for the samples of X. If `X` was a single sample, then the output is a single class label. If `X` was a set of samples, then the output is a `numpy.array` of dimensions `(n,`)<br>\n",
    "    \n",
    "    *Note:*<br>\n",
    "    The provided code outline already handles the case where `X` is a set of samples (by running on each sample separately). You need write the main logic of this function, i.e. calculating the prediction for a single sample.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self, k = 5, distance = euclidean_distance):\n",
    "        \n",
    "        self.k = k\n",
    "        self.distance = distance #constructor to set values of the class\n",
    "       \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y #function to fit the training data \n",
    "\n",
    "        return self # this should probably be the last line in your fit function\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # the fololwing block of code handles lists/arrays containing multiple samples\n",
    "        X = np.array(X)\n",
    "        if len(X.shape) >= 2:\n",
    "            return np.array([self.predict(x) for x in X])\n",
    "        \n",
    "        # in the following section, you can assume that X is a single n-dimensional sample:\n",
    "        self.X = X #test dataset\n",
    "        distances = {} #dictionary that will contain each index and the distance corresponding to that index\n",
    "        neighbours = []\n",
    "        for i in range(len(self.X_train)):\n",
    "            distances[i] = self.distance(self.X_train[i], X) #uses the euclidian distance function to check distance \n",
    "            #between new sample and sample at point i\n",
    "        sortDist = sorted(distances.items(), key = lambda x:x[1]) #sorts the distances from smallest to largest\n",
    "        for i in range(self.k):\n",
    "            neighbours.append(sortDist[i][0]) #gets the k number of shortest distances (which are therefore neighbours)\n",
    "        classes = []\n",
    "        for i in range(len(neighbours)):\n",
    "            classes.append(self.y_train[neighbours[i]]) #neighbours[i] will get the index of a nearest neighbour\n",
    "            #the class of that nearest neighbour is then appended\n",
    "        return most_common_class(classes) #finds the common class from the nearest neighbours and assigns sample to this class\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [-2.45500e+00  7.00000e-02  2.34400e+00  2.60000e+01  3.21300e+00\n",
      "  3.09993e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.35200e+00  8.30000e-02  1.36700e+00  2.80000e+01  2.93500e+00\n",
      "  3.57496e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.55300e+00  7.30000e-02  2.64900e+00  3.20000e+01  2.87700e+00\n",
      "  3.73622e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.43900e+00  5.70000e-02  2.30000e-01  1.50000e+01  3.02400e+00\n",
      "  1.77312e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: triclinic\n",
      "Sample: [-2.88700e+00  4.00000e-02  3.14400e+00  5.20000e+01  2.69000e+00\n",
      "  6.79101e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.56400e+00  5.80000e-02  2.73000e+00  2.80000e+01  2.85600e+00\n",
      "  3.60121e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.91100e+00  6.40000e-02  3.07900e+00  3.40000e+01  2.63300e+00\n",
      "  4.31422e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: triclinic\n"
     ]
    }
   ],
   "source": [
    "# Initialise the classifier with k=3 (using 9 neigbours)\n",
    "classifier = knn(k=9)\n",
    "# Fit the classifier to the training set\n",
    "classifier.fit(X_nd_train, y_nd_train)\n",
    "\n",
    "# Predict the classification for some samples to the testing set\n",
    "for (sample, prediction, truth) in zip(X_nd_test[3:10], classifier.predict(X_nd_test[3:10]), y_nd_test[3:10]):\n",
    "    # Print the sample, predicted class and ground truth\n",
    "    print(\"Sample: {}\\n\\tPredicted class: {}\\n\\tTrue class: {}\".format(sample, prediction, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For the second part of the assignment, you are expected to implement a decision tree classifier. A decision tree models the data as a series of decisions based on the values of different features of the sample. The predictions are made in an interpretable fashion, where the decision is made by considering different sample features, and their values, until the new sample is grouped with similar samples presented during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 - Entropy and Information Gain\n",
    "#### 10% marks\n",
    "\n",
    "Important concepts in decision trees are **entropy** and **information gain**. **Entropy** is a measure of the variance in the data, of how \"unpredictable\" the dataset is:\n",
    "\n",
    "- If a dataset consists only of samples with the same label (e.g. $\\{\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare\\}$), the entropy will be zero (the data is predictable).\n",
    "- If a dataset consists of an equal amount of samples from two classes, (e.g. $\\{\\blacksquare,\\triangle,\\triangle,\\blacksquare,\\triangle,\\blacksquare\\}$), the entropy will be one (the data is unpredictable).\n",
    "- If the class distribution in the dataset lies somewhere between these two extremes, the entropy will be between one and zero.\n",
    "- Entropy also increases as the number of different classes increases.\n",
    "\n",
    "Given $c$ classes, and their associated probabilities (frequencies) in the datasets, $p_i \\forall i \\in 0..c$, the entropy is calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "E = -\\sum_i p_i log_2(p_i).\n",
    "\\end{equation*}\n",
    "\n",
    "Implement a function `entropy` which calculates the entropy of a set of class labels from the list of their associated probabilities:\n",
    "\n",
    "- `entropy(p_per_class)`: calculates the entropy of a set of class probabilities\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `p_per_class`: A list of $c$ probabilities, one for each class in the dataset. This either a list of length $c$, or a `numpy.array` of dimensions `(c,)`<br>\n",
    "\n",
    "    *Returns:*<br>\n",
    "    The entropy of a set with class probabilities given in `p_per_class`.<br>\n",
    "    \n",
    "    *Note:*<br>\n",
    "    You do not need to check if the list of probabilities given is correct; i.e. you may assume that all the probabilities in the list are strictly greater than zero, and that the sum of all the class probabilities equals one.\n",
    "    \n",
    "    *Example:*\n",
    "\n",
    "    ```python\n",
    "    probabilities = [0.5, 0.5]\n",
    "    print(entropy(probabilities))\n",
    "    ```\n",
    "    *Example output:*<br>\n",
    "    `1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p_per_class):\n",
    "    if len(p_per_class) == 1: #if there is only one sample in the list then calculate the entropy  for one sample\n",
    "        return -(p_per_class[0] * math.log2(p_per_class[0])) #entropy formula\n",
    "    elif len(p_per_class) > 2: #if the list is greater than 2 then it needs to be split \n",
    "        midpoint = int(len(p_per_class) / 2) #find midpoint of the list\n",
    "        return entropy(p_per_class[0:midpoint]) + entropy(p_per_class[midpoint:len(p_per_class)]) #recursive\n",
    "        #above statement will work out the entropy for the left hand side list and add it to the entropy of the right hand side list\n",
    "    else:\n",
    "        positive = p_per_class[0]\n",
    "        negative = p_per_class[1] #works out entropy for two values\n",
    "        return -(positive * math.log2(positive) + (negative * math.log2(negative))) #formula negative sum of two entropies\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of a set with only one class: -0.0\n",
      "Entropy of a set with two equally probable classes: 1.0\n",
      "Entropy of a set with four equally probable clasess: 2.0\n",
      "Entropy of a set with four classes with different probabilities: 1.846\n",
      "Entropy of the numerical (batteries) dataset: 1.552\n",
      "Entropy of the categorical (forest cover) dataset: 1.739\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of a set with only one class: {}\".format(entropy([1.0])))\n",
    "print(\"Entropy of a set with two equally probable classes: {}\".format(entropy([0.5, 0.5])))\n",
    "print(\"Entropy of a set with four equally probable clasess: {}\".format(entropy([0.25, 0.25, 0.25, 0.25])))\n",
    "print(\"Entropy of a set with four classes with different probabilities: {:.3f}\".format(\n",
    "    entropy([0.2, 0.3, 0.4, 0.1])))\n",
    "print(\"Entropy of the numerical (batteries) dataset: {:.3f}\".format(\n",
    "    entropy(list(class_probabilities(y_nd).values()))))\n",
    "print(\"Entropy of the categorical (forest cover) dataset: {:.3f}\".format(\n",
    "    entropy(list(class_probabilities(y_cd).values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **information gain** describes how much variance was lost by dividing a set into parts, i.e. how much more \"predictable\" the parts are from the whole.\n",
    "\n",
    "For example, if we have a dataset $d=\\{\\blacksquare,\\triangle,\\triangle,\\blacksquare,\\triangle,\\blacksquare,\\triangle,\\blacksquare\\}$ (which is \"maximally unpredictable\", so we can calculate $E(d) = 1$), we can:\n",
    "- split it into $d_1=\\{\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare\\}$ and $d_2=\\{\\triangle, \\triangle, \\triangle, \\triangle\\}$.\n",
    "\n",
    "    Both of $d_1$ and $d_2$ are \"very predictable\" ($E(d_1) = E(d_2) = 0$), both subsets consist of one class only). The resulting information gain is large (we go from unpredictable to predictable, so we gain information):\n",
    "    \n",
    "    $I = E(d) - (0.5E(d_1) + 0.5E(d_2) = 1 - (0+0) =1$\n",
    "    \n",
    "- split it into $d_1=\\{\\blacksquare,\\triangle,\\triangle,\\blacksquare\\}$ and $d_2=\\{\\triangle,\\blacksquare,\\triangle,\\blacksquare\\}$.\n",
    "\n",
    "    Both of $d_1$ and $d_2$ are \"very unpredictable\" ($E(d_1) = E(d_2) = 1$), both subsets have the same amount of samples from each of the two classes). The resulting information gain is small (we start from an unpredictable dataset, and end with two unpredictable subsets, so we do not gain information):\n",
    "\n",
    "    $I = E(d) - (0.5E(d_1) + 0.5E(d_2) = 1 - (0.5\\times1+0.5\\times1) =0$\n",
    "    \n",
    "<br>\n",
    "In general, if we have a dataset $d$ containing $|d|$ samples, and we split it into $s$ subsets $d_i \\forall i=1..s$, each of size $|d_i|$, the information gain of this split is calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    I = E(d) - \\sum_{i=0}^s \\frac{|d_i|}{|d|}E(d_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 - Decision Tree classifier\n",
    "#### 30% marks\n",
    "\n",
    "In each decision node of a decision tree, this classifier splits the dataset into two or more parts according to a value of a certain feature in the dataset.\n",
    "\n",
    "Given a set of M samples $\\mathbf{x} = \\{\\mathbf{x}_i\\} \\forall i=1..M$ with n features each $\\mathbf{x}_i = \\{x_{1,i}, \\dots, x_{n,i}\\}$, we could decide to split the dataset at the $j^{\\texttt{th}}$ (numerical) feature at value $v$. This would mean splitting the dataset into:\n",
    "- the subset $\\mathbf{x_{<}} = \\{\\mathbf{x}_i | x_{j,i} < v\\}$, which contains all the samples with the $j^{\\texttt{th}}$ smaller than $v$ and\n",
    "- the subset $\\mathbf{x_{\\geq}} = \\{\\mathbf{x}_i | x_{j,i} \\geq v\\}$, which contains all the samples with the $j^{\\texttt{th}}$ greater or equal than $v$.\n",
    "\n",
    "Similarly, we could decide to split a dataset at the $j^{\\texttt{th}}$ feature which is categorical. If the $j^{\\texttt{th}}$ feature of a sample $\\mathbf{x}_i$ can hold values $\\{\\texttt{child}, \\texttt{teen}, \\texttt{adult}\\}$, the resulting subsets would be:\n",
    "- the subset $\\mathbf{x_{\\texttt{child}}} = \\{\\mathbf{x}_i | x_{j,i} = \\texttt{child}\\}$, which contains all the samples with the feature $j^{\\texttt{th}}$ equal to $\\texttt{child}$, \n",
    "- the subset $\\mathbf{x_{\\texttt{teen}}} = \\{\\mathbf{x}_i | x_{j,i} = \\texttt{teen}\\}$, which contains all the samples with the feature $j^{\\texttt{th}}$ equal to $\\texttt{teen}$ and\n",
    "- the subset $\\mathbf{x_{\\texttt{adult}}} = \\{\\mathbf{x}_i | x_{j,i} = \\texttt{adult}\\}$, which contains all the samples with the feature $j^{\\texttt{th}}$ equal to $\\texttt{adult}$.\n",
    "\n",
    "In each node of the decision tree, the dataset is split according to the attribute $j$ which results in **the highest information gain** after the split. For categorical features, there is only one possible split (each category becomes a subset). For numerical features, it is neccessary to check all the possible values of split. For example, if the $j^{\\texttt{th}}$ in the dataset has appeared with the values of $\\{1, 1.5, 2.7, 3.2, 5\\}$, you need to calculate the information gain for splitting the dataset at $v=1.5$ (into $\\mathbf{x_{< 1.5}}$ and $\\mathbf{x_{\\geq 1.5}}$), $v=2.7$, $v=3.2$, $v=5$.\n",
    "\n",
    "The splitting continues until each node contains only samples of a single class, or a stopping criterion is reached. If a node contains training samples of more than one class, it returns the label of the majority (most probable) class as its decision.\n",
    "\n",
    "Implement a `DecisionTree` classifier which works on samples with both numerical and categorical features as a Python class implementing the following member functions:\n",
    "\n",
    "- `__init__(self, max_depth = -1, min_samples_per_node = 1)`: class constructor\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `max_dept` : The maximal depth of the tree (`max_depth == 0` corresponds to a decision tree with only the root node. `max_depth == -1` means no limit to the depth of the tree) <br>\n",
    "    `min_samples_per_node`: The minimal number of samples contained in a terminal (decision) node. If a split would cause one of the subsets to have less than `min_samples_per_node` samples, the split is not performed. <br><br>\n",
    "    \n",
    "- `fit(self, X, y)`: member function used to train the classifier (fit the model to the data)\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The dataset samples. This is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, n)`<br>\n",
    "    `y`: The dataset labels. This is either a list of length $M$, or a `numpy.array` of dimensions `(M,)`<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    The trained classifier model.<br><br>\n",
    "    \n",
    "- `predict(self, X)`: member function used to predict the classes of samples `X`\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The samples for which to predict a class. If this is a single sample, then `X` is either a list of length $n$, or a `numpy.array` of dimensions `(n,)`. If this is a set of samples, then `X` is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, N)`<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    Predicted class for the samples of X. If `X` was a single sample, then the output is a single class label. If `X` was a set of samples, then the output is a `numpy.array` of dimensions `(n,`)<br>\n",
    "    \n",
    "    *Note:*<br>\n",
    "    The provided code outline already handles the case where `X` is a set of samples (by running on each sample separately). You need write the main logic of this function, i.e. calculating the prediction for a single sample.<br><br>\n",
    "    \n",
    "*Note:* Your implementation will be tested in two parts. Firstly, it will be tested only on a dataset containing numerical values. Secondly, it will also be tested on a dataset containing a mix of categorical and numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following classes and functions were created using this tutorial: Assembly AI (2022) How to implement decision trees from scratch with python [video]. Available from https://www.youtube.com/watch?v=NxEHSAfFlK8&t=974s [accessed 13 December 2022]\n",
    "class Node:\n",
    "    def __init__(self, feature = None, threshold = None, left = None, right = None, *, value = None): #asterisk means value must be passed by name\n",
    "        self.feature = feature #feature that was used to divide the node\n",
    "        self.threshold = threshold #specific threshold that was used to divide the node\n",
    "        self.left = left #left tree of node\n",
    "        self.right = right #right tree of node\n",
    "        self.value = value #value that will be assigned\n",
    "        #constructor of the class\n",
    "    def isLeaf(self):\n",
    "        return self.value is not None #if a value is assigned to a node then it means a leaf node has been created\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth = -1, min_samples_per_node = 1):\n",
    "        self.max_depth = max_depth; #constructor of the class assigned the depth of the tree and the number of samples for each node\n",
    "        self.min_samples_per_node = min_samples_per_node\n",
    "        self.root = None #access of root node\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y #decision tree fitted to the training data\n",
    "        self.root = self.grow(X, y) #function used to create the decision tree itself, which becomes the root node\n",
    "        \n",
    "    def grow(self, X, y, depth = 0):\n",
    "        samples, features = X.shape #shape will get number of rows and columns, but we are only really interested in columns\n",
    "        labels = len(np.unique(y)) #gets the number of unique classes\n",
    "        if (depth == self.max_depth or labels == 1 or samples < self.min_samples_per_node): \n",
    "            #a leaf node will be made if the current depth is the max_depth, the number of labels in the node is 1\n",
    "            #or the number of samples is less than the minimum samples per node\n",
    "            leaf = most_common_class(y) #finds the class of the leaf node\n",
    "            #as most common class is returning the first value in a tie this could lead to varying results\n",
    "            return Node(value=leaf) #creates a leaf node object with the class value assigned\n",
    "        featuresIndex = range(features) #creates a list of column indexes for splitting\n",
    "        best_feature, best_threshold = self.best_split(X, y, featuresIndex)\n",
    "        left, right = self.split(X[:, best_feature], best_threshold)\n",
    "        left = self.grow(X[left, :], y[left], depth + 1)\n",
    "        right = self.grow(X[right, :], y[right], depth + 1)\n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "    \n",
    "    def best_split(self, X, y, features):\n",
    "        best_gain = -1 \n",
    "        split_index, split_threshold = None, None #best gain, split_index and split_threshold created and will be assigned to later\n",
    "        for feature in features: #for each index in the list of indexes\n",
    "            col = X[:, feature] #get the values in the column the index points towards\n",
    "            thresholds = np.unique(col) #find what values are unique in the column\n",
    "            for threshold in thresholds: #for each unique value\n",
    "                gain = self.infoGain(y, col, threshold) #find information gain for each threshold\n",
    "                if gain > best_gain: #if the gain found is better than the existing gain, best_gain becomes the gain calculated\n",
    "                    best_gain = gain\n",
    "                    split_index = feature #holds the best index for splitting\n",
    "                    split_threshold = threshold #holds the best threshold for splitting\n",
    "        return split_index, split_threshold\n",
    "    \n",
    "    def infoGain(self, y, col, thres):\n",
    "        parentProb = class_probabilities(y) #find all probabilities for each class for the whole of y\n",
    "        parentList = list(parentProb.values()) #change the returned dictionary to a list\n",
    "        parentEntropy = entropy(parentList) #calculate the entropy for the whole of dataset y\n",
    "        left, right = self.split(col, thres) #split dataset based on thrsehold value given\n",
    "        if (len(left) == 0 or len(right) == 0): #if one list is null then return\n",
    "            return 0\n",
    "        samples = len(y) #number of total samples\n",
    "        leftSamples, rightSamples = len(left), len(right) #number of samples in the left and right trees\n",
    "        probLeft, probRight = class_probabilities(y[left]), class_probabilities(y[right]) #find all probabilities for classes in the left and right trees\n",
    "        listLeft, listRight = list(probLeft.values()), list(probRight.values()) #convert to lists\n",
    "        entropyLeft, entropyRight = entropy(listLeft), entropy(listRight) #gets the entropy of both branches\n",
    "        childEntropy = (leftSamples/samples) * entropyLeft + (rightSamples/samples) * entropyRight #entropy of a child node is\n",
    "        #number of samples in left branch/total samples * left branch entropy + number of samples in right branch/total samples * right branch entropy\n",
    "        gain = parentEntropy - childEntropy #information gain is parent entropy subtracted by child entropy\n",
    "        return gain\n",
    "    \n",
    "    def split(self, col, thres):\n",
    "        if isinstance(col, str): #splits for categorical values\n",
    "            lefty = np.argwhere(col == thres).flatten() #returns all values where the column is equal to the threshold \n",
    "            righty = np.argwhere(col != thres).flatten() #returns all values where the column is not equal to threshold\n",
    "            #this may be wrong as the split becomes left for one categorical value and right for all other categorical values\n",
    "        else: #splits for numerical values\n",
    "            lefty = np.argwhere(col <= thres).flatten() #returns all values where the column is less than or equal to the split threshold\n",
    "            righty = np.argwhere(col > thres).flatten() #returns all values where the column is greater than the split threshold\n",
    "        #flatten creates the format as one list as opposed to multiple lists, aargwhere returns all values that satisfy the decision in the paranthesis\n",
    "        return lefty, righty\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse(x, self.root) for x in X]) #for every value in X, go through the tree\n",
    "    \n",
    "    def traverse(self, x, node):\n",
    "        if node.isLeaf():\n",
    "            return node.value #if the end of the tree is reached return the value in the tree\n",
    "        \n",
    "        if x[node.feature] <= node.threshold: #if the feature is smaller than the threshold value then go across the left tree\n",
    "            return self.traverse(x, node.left)\n",
    "        else: #otherwise go on the right tree\n",
    "            return self.traverse(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELLS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will test if the classifier implemented model can fit to the [numerical dataset](#Numerical-data), and predict solutions for some of the testing samples (this is not model evaluation yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [-2.45500e+00  7.00000e-02  2.34400e+00  2.60000e+01  3.21300e+00\n",
      "  3.09993e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.35200e+00  8.30000e-02  1.36700e+00  2.80000e+01  2.93500e+00\n",
      "  3.57496e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.55300e+00  7.30000e-02  2.64900e+00  3.20000e+01  2.87700e+00\n",
      "  3.73622e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.43900e+00  5.70000e-02  2.30000e-01  1.50000e+01  3.02400e+00\n",
      "  1.77312e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: triclinic\n",
      "Sample: [-2.88700e+00  4.00000e-02  3.14400e+00  5.20000e+01  2.69000e+00\n",
      "  6.79101e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.56400e+00  5.80000e-02  2.73000e+00  2.80000e+01  2.85600e+00\n",
      "  3.60121e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.91100e+00  6.40000e-02  3.07900e+00  3.40000e+01  2.63300e+00\n",
      "  4.31422e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: triclinic\n"
     ]
    }
   ],
   "source": [
    "# Initialise the classifier, requiring at least 3 samples in each node\n",
    "classifier = DecisionTree(min_samples_per_node = 3)\n",
    "# Fit the classifier to the training data\n",
    "classifier.fit(X_nd_train, y_nd_train)\n",
    "# Predict the classification for some samples to the testing set\n",
    "for (sample, prediction, truth) in zip(X_nd_test[3:10], classifier.predict(X_nd_test[3:10]), y_nd_test[3:10]):\n",
    "    # Print the sample, predicted class and ground truth\n",
    "    print(\"Sample: {}\\n\\tPredicted class: {}\\n\\tTrue class: {}\".format(sample, prediction, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will check if the dataset is able to fit the model to the [dataset with mixed numerical and categorical values](#Categorical-data), and predict solutions for some of the testing samples (this is not model evaluation yet):\n",
    "\n",
    "_**Warning:**_ Executing this cell may take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [2.4 48.0 11.0 0.0 0.0 0.4 224.0 215.0 123.0 1.3 'Cache_la_Poudre'\n",
      " 'ELU-2717']\n",
      "\tPredicted class: Ponderosa_Pine\n",
      "\tTrue class: Cottonwood/Willow\n",
      "Sample: [3.3 67.0 16.0 0.1 0.0 1.7 234.0 207.0 100.0 1.9 'Rawah' 'ELU-8772']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [3.3 60.0 15.0 0.1 -0.0 2.4 231.0 207.0 104.0 4.1 'Neota' 'ELU-8703']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [3.2 62.0 10.0 0.4 0.0 2.0 229.0 219.0 122.0 0.9 'Comanche_Peak'\n",
      " 'ELU-7755']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [2.9 121.0 6.0 0.5 0.1 3.6 230.0 235.0 139.0 3.1 'Rawah' 'ELU-4744']\n",
      "\tPredicted class: Lodgepole_Pine\n",
      "\tTrue class: Lodgepole_Pine\n",
      "Sample: [3.2 155.0 15.0 0.2 0.0 0.4 237.0 240.0 129.0 2.3 'Neota' 'ELU-4758']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [3.0 330.0 8.0 0.1 0.0 2.6 201.0 231.0 169.0 6.2 'Rawah' 'ELU-7745']\n",
      "\tPredicted class: Lodgepole_Pine\n",
      "\tTrue class: Spruce/Fir\n"
     ]
    }
   ],
   "source": [
    "# Initialise the classifier, requiring the decision tree to have no more than 5 levels\n",
    "classifier = DecisionTree(max_depth = 4)\n",
    "# Fit the classifier to the training data\n",
    "classifier.fit(X_cd_train, y_cd_train)\n",
    "\n",
    "# Predict the classification for some samples to the testing set\n",
    "for (sample, prediction, truth) in zip(X_cd_test[3:10], classifier.predict(X_cd_test[3:10]), y_cd_test[3:10]):\n",
    "    # Print the sample, predicted class and ground truth\n",
    "    print(\"Sample: {}\\n\\tPredicted class: {}\\n\\tTrue class: {}\".format(sample, prediction, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and analysis\n",
    "\n",
    "Executing all the cells from the previous section correctly only means that your implementation of [kNN classifier](#knn-Classifier) and [Decision Tree classifier](#Decision-Tree-classifier) runs on the provided data. This has still not provided any insight on how well those models represent the data.\n",
    "\n",
    "In this section, you are instead required to calculate some evaluation metrics, and evaluate the produced models on the whole of the held-out data (test set) defined in the [Dataset](#Dataset) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 - Evaluation\n",
    "#### 10% marks\n",
    "\n",
    "You are required to write a function that will calculate different evaluation metrics based on the ground truth labels and the predicted labels. Specifically, write a function:\n",
    "- `evaluate(y_true, y_pred)`: evaluate classification results\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `y_true` : Ground truth labels for $n$ samples. This is either a list of $n$ elements, or a `numpy.array` of dimensions `(n,)`\n",
    "    `y_pred` : Predicted labels for $n$ samples. This is either a list of $n$ elements, or a `numpy.array` of dimensions `(n,)`\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    A touple `(a, p, r, k)` consisting of the following values:\n",
    "    - `a` : overall accuracy of the predictions\n",
    "    - `p` : a dictionary containing a precision score for each class present in `y_true`\n",
    "    - `r` : a dictionary containing a recall score for each class present in `y_true`\n",
    "    - `k` : Cohen's Kappa metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precisionDict = {}\n",
    "    recallDict = {} #two dictionaries to return precision and recall for class\n",
    "    values = len(np.unique(y_true)) #finds the number of unique labels needed for key values in dictionary\n",
    "    labels = np.unique(y_true) #finds the unique labels needed for key values in dictionary\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels = np.unique(y_true))\n",
    "    #sklearn function to find precision, recall, fscore and support but we will only use precision and recall\n",
    "    #support would return the number of occurances in y_true (Sklearn.metrics.precision_recall_fscore_support (undated) scikit. Available from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html [accessed 17 December 2022].\n",
    "    #fscore is a weighted harmonic mean of the precision and recall (Sklearn.metrics.precision_recall_fscore_support (undated) scikit. Available from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html [accessed 17 December 2022].\n",
    "    #returns a list for each\n",
    "    for i in range(values):\n",
    "        precisionDict[labels[i]] = precision[i] #assigns the precision to the corresponding label\n",
    "        recallDict[labels[i]] = recall[i] #assigns the recall to the corresponding label\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred) #sklearn function that calculates accuracy\n",
    "    cohenKappa = metrics.cohen_kappa_score(y_true, y_pred) #sklearn function that calculates cohen's kappa\n",
    "    return accuracy,precisionDict,recallDict,cohenKappa\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your implementation of [kNN classifier](#Task-2.2---kNN-classifier) and [Decision Tree classifier](#Task-3.2---Decision-Tree-classifier) on the numerical dataset defined in the [Dataset](#Dataset) section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating numerical dataset, using kNN:\n",
      "Accuracy = 0.5833, Cohen's Kappa = 0.3651\n",
      "\tClass monoclinic: precision = 0.56, recall = 0.62\n",
      "\tClass orthorhombic: precision = 0.67, recall = 0.60\n",
      "\tClass triclinic: precision = 0.50, recall = 0.50\n",
      "Evaluating numerical dataset, using DecisionTree:\n",
      "Accuracy = 0.5417, Cohen's Kappa = 0.2997\n",
      "\tClass monoclinic: precision = 0.43, recall = 0.62\n",
      "\tClass orthorhombic: precision = 0.62, recall = 0.50\n",
      "\tClass triclinic: precision = 0.67, recall = 0.50\n"
     ]
    }
   ],
   "source": [
    "classifier_knn = knn(k=5)\n",
    "classifier_knn.fit(X_nd_train, y_nd_train)\n",
    "\n",
    "classifier_dt = DecisionTree(min_samples_per_node = 3)\n",
    "classifier_dt.fit(X_nd_train, y_nd_train)\n",
    "\n",
    "y_knn = classifier_knn.predict(X_nd_test)\n",
    "y_dt = classifier_dt.predict(X_nd_test)\n",
    "\n",
    "a_knn, p_knn, r_knn, k_knn = evaluate(y_nd_test, y_knn)\n",
    "a_dt, p_dt, r_dt, k_dt = evaluate(y_nd_test, y_dt)\n",
    "\n",
    "print(\"Evaluating numerical dataset, using kNN:\")\n",
    "print(\"Accuracy = {:.4f}, Cohen's Kappa = {:.4f}\".format(a_knn, k_knn))\n",
    "for key in p_knn.keys():\n",
    "    print(\"\\tClass {}: precision = {:.2f}, recall = {:.2f}\".format(key, p_knn[key], r_knn[key]))\n",
    "\n",
    "print(\"Evaluating numerical dataset, using DecisionTree:\")\n",
    "print(\"Accuracy = {:.4f}, Cohen's Kappa = {:.4f}\".format(a_dt, k_dt))\n",
    "for key in p_dt.keys():\n",
    "    print(\"\\tClass {}: precision = {:.2f}, recall = {:.2f}\".format(key, p_dt[key], r_dt[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your implementation of [Decision Tree classifier](#Decision-Tree-classifier) on the [dataset containing a mixture of numerical and categorical data](#Categorical-data):\n",
    "\n",
    "_**Warning:**_ Executing this cell may take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating categorical dataset, using DecisionTree:\n",
      "Accuracy = 0.6987, Cohen's Kappa = 0.5061\n",
      "\tClass Aspen: precision = 0.00, recall = 0.00\n",
      "\tClass Cottonwood/Willow: precision = 0.00, recall = 0.00\n",
      "\tClass Douglas-fir: precision = 0.46, recall = 0.05\n",
      "\tClass Krummholz: precision = 0.68, recall = 0.52\n",
      "\tClass Lodgepole_Pine: precision = 0.74, recall = 0.75\n",
      "\tClass Ponderosa_Pine: precision = 0.64, recall = 0.86\n",
      "\tClass Spruce/Fir: precision = 0.67, recall = 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classifier_categorical = DecisionTree(max_depth = 4)\n",
    "classifier_categorical.fit(X_cd_train, y_cd_train)\n",
    "\n",
    "y_cd_pred = classifier_categorical.predict(X_cd_test)\n",
    "a_cat, p_cat, r_cat, k_cat = evaluate(y_cd_test, y_cd_pred)\n",
    "\n",
    "print(\"Evaluating categorical dataset, using DecisionTree:\")\n",
    "print(\"Accuracy = {:.4f}, Cohen's Kappa = {:.4f}\".format(a_cat, k_cat))\n",
    "for key in p_cat.keys():\n",
    "    print(\"\\tClass {}: precision = {:.2f}, recall = {:.2f}\".format(key, p_cat[key], r_cat[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 - Analysis\n",
    "#### 10% marks\n",
    "\n",
    "Answer the following questions in the space provided below (marked as **YOUR ANSWERS**):\n",
    "1. Explain the difference between the different performance metrics calculated for [Task 4.1](#Task-4.1---Evaluation). Which additional metrics could you propose to evaluate the performance of the defined models? Which of the calculated metrics would you chose to report, and why, for the analysis of the two [datasets](#Datasets) used in this assessment?\n",
    "\n",
    "2. In case you were handling a _very large_ amount of data, which of the models do you expect to be larger (i.e. take up more memory)? Which of the models do you expect to train faster? Which of the models do you expect to reach a decision faster? Explain and justify your answers.\n",
    "\n",
    "3. When handling numerical data, what is the role of normalisation? The numerical data used in this assessment was not normalised. If you were to normalise the data before training these classification models, would either of them change how they represent the data (and potentially return different results)?\n",
    "\n",
    "4. For evaluating the performance of the classifiers implemented for this assignment, $80\\%$ of the data was used for training the models, and $20\\%$ for testing. What are the shortcomings of this evaluation strategy, and can you propose and describe a better one?\n",
    "\n",
    "_Note:_ You may use the cell marked as **EXPERIMENTAL CELL** to write any code that could help you answer the above questions, or in general, to experiment with the code produced for this assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTAL CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR ANSWERS:\n",
    "\n",
    "1. <font color='red'>Answer to question 1\n",
    "\n",
    "Using kNN for the batteries dataset returned an accuracy of 0.5833, a Cohen's Kappa of 0.3651 and the precision and recall for monoclinic, orthorhombic and triclinic was 0.56 and 0.62, 0.67 and 0.60 and 0.5 and 0.5 respectively.\n",
    "For the decision tree, the accuracy was 0.5417 and Cohen's Kappa was 0.2997. For monoclinic, orthorhombic and triclinic the precision and recall were 0.43 and 0.62, 0.62 and 0.5 and 0.67 and 0.5 respectively. Therefore, overall we could say that kNN was more accurate than a decision tree, with accuracy being True Positives and True Negatives / Total. Precision is True Positives / Number of Predicted Yeses. On average this was 0.577 (3sf) for kNN and 0.573 (3sf) for a decision tree, meaning kNN is more precise. Recall is True Positives / Actual Number of Yeses which for kNN was an average of 0.573 (3sf) compared to 0.54 for a decision tree, meaning kNN was better at recall. Finally, Cohen's Kappa is defined as \"the level of agreement between two raters or judges who each classify items into mutually exclusive categories.\" (Zach, 2021). Again kNN produces a higher score, this time for Cohen's Kappa, showing that there is a better level of agreement between the model and the true values.\n",
    "\n",
    "Additional metrics that could have been used are specificity, which evaluates the number of True Negatives / Actual Number of Nos, the false positive rate which is the False Positives / Actual Number of Nos (however this is just 1 - specificity) or the misclassification rate, which is the number of False Positives and False Negatives / Total \n",
    "(however this is 1 - accuracy). Overall, the better model was kNN as it outperformed decision trees for each metric. This could be because the batteries dataset had a max_depth of -1, which would mean that there was no stopping condition for depth. This could have led to the tree overfitting the data as the tree would be very deep and so would only be memorising the training data as opposed to working on unseen examples.\n",
    "\n",
    "Of all the metrics that I would choose to report, I would choose recall. This is because recall can be used to plot an ROC curve, which is a way to determine how good the models are at classifying data. By obtaining both recall and the false positive rate, we could plot an ROC curve by having recall on y and false positive rate on x. It would also be possible to change the values of k in kNN together with the depth in the decision tree to obtain different recall values (and false positive rate values) and then plot these on the ROC curve. We could then work out the area under the graph. The closer the area under the graph is to 1, the better the model is at predicting data (Shaneyfelt, 2019). This could mean that, on average, the decision tree could be a better classifier overall if it's area under the curve that was closer to 1 and thus is better at predicting data.  \n",
    "\n",
    "References:\n",
    "\n",
    "Terry Shaneyfelt (2019) How to interpret ROC curves [video]. Available from https://www.youtube.com/watch?v=egTNM8NSa2k [accessed 17 December 2022]. \n",
    "Zach (2021) Cohen's kappa statistic: Definition & Example, Statology. Available from https://www.statology.org/cohens-kappa-statistic/ [accessed 17 December 2022]. </font> \n",
    "\n",
    "2. <font color='red'>Answer to question 2\n",
    "\n",
    "KNN is a non-parametric model. This means that no assumptions are made about the data and that the number of parameters is equal to the number of data points. As a result it does not scale well. As the number of data samples increases so the model will need to look at every single data point in order to obtain the k best samples. As the sample size increases, the model will take a larger amount of time to analyize all data points. Equally, decision trees are also non-parametric and, due to being created recursively they will have a \"memory complexity of O(N)\" (StackOverflow, 2017). However, kNN does have a \"space complexity of O(Nd)\" (Kumar, 2021) where \"d is dimensionality\" (Kumar, 2021) which simplifies to O(N). Since there is a scalar involved with kNN, kNN will likely use more memory for very large datasets than the recursive decision tree. \n",
    "    \n",
    "The model that trains fastest is kNN. This is because it has a \"time complexity of O(kNd)\" (Kumar, 2021) where k is number of neighbours and d is \"dimensionality of the data\" (Kumar, 2021). Decision trees have a \"time complexity of O(dNlogN)\" (Kumar, 2021) where d is \"the dimensionality of the data\" (Kumar, 2021). Simplified this means that kNN is O(N) and decision trees are O(NlogN) which in turn means that kNN has the smaller time complexity and therefore trains faster. During runtime, the time complexity for decision trees will be \"O(d) where d is the depth of the tree\" (Kumar, 2021). This would essentially end up being O(N) complexity. Equally time complexity at runtime for kNN is \"O(nd+kn) runtime or O(ndk) runtime\" (StackExchange, 2016) which would again simplify to O(N). The model that reaches a decision faster will be determined by whether the number of samples for kNN is less than the depth of the tree in the decision tree. If kNN has fewer samples (and the values of k and d are low) a decision will be reached quicker to predict a class. If not, the decision tree will predict the class quicker. However, as it's unlikely that the depth of the tree would be greater than the number of samples in kNN, it's more likely that the decision tree will be quicker.\n",
    "\n",
    "References:\n",
    "\n",
    "Kumar, P. (2021) Time complexity of ML Models, Medium. Analytics Vidhya. Available from https://medium.com/analytics-vidhya/time-complexity-of-ml-models-4ec39fad2770 [accessed 17 December 2022].\n",
    "StackExchange (2016) k-NN computational complexity Stack Exchange. Available from https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity [accessed 19 December 2022].\n",
    "StackOverflow (2017) Space complexity of recursive function Stack Overflow. Available from https://stackoverflow.com/questions/43298938/space-complexity-of-recursive-function [accessed 17 December 2022].\n",
    " </font> \n",
    "\n",
    "3. <font color='red'>Answer to question 3\n",
    "\n",
    "The role of normalisation in numerical data is \"the process of casting the data to the specific range, like between 0 and 1\" (Ali et all, 2014, p1) and is specifically done \"when there are big differences in the ranges of different features\" (Ali et all, 2014, p1). The way to normalize data for machine learning is \"xnorm = (xi  xmin) / (xmax  xmin)\" (Zach, 2021) and is used when there are no outliers in the data (Ali et all, 2014, p1). There is another method known as Z-Score Standardization which assumes a normal distribution of data and is worked out by \"(x[i]-mean of x)/standard deviation of x\" (Ali et all, 2014, p5). Data should be normalized as \"normalization ensures that no one variable may influence the performance of the model in a particular way simply due to the fact that it has larger numbers.\" (Croydon Early Learning, 2022). If, therefore, there were certain values in the dataset that were higher than other values, it could decrease the model's accuracy, so by normalizing the data any discrepancies are removed while making sure the relationship of the data is still the same. This may create better models and therefore more accurate results.\n",
    "\n",
    "References:\n",
    "\n",
    "Ali, P.J.M., Faraj, R.H., Koya, E., Ali, P.J.M. and Faraj, R.H., 2014. Data normalization and standardization: a technical report. Mach Learn Tech Rep, 1(1), pp.1-6. [accessed 17 December 2022].\n",
    "Kiligann, A. (2022) Why do we normalize data in machine learning?, Croydon Early Learning. Available from https://croydonearlylearning.com.au/education/why-do-we-normalize-data-in-machine-learning.html [accessed 17 December 2022]. \n",
    "Zach (2021) How to normalize data in python, Statology. Available from https://www.statology.org/normalize-data-in-python/ [accessed 17 December 2022]. </font> \n",
    "\n",
    "4. <font color='red'>Answer to question 4\n",
    "\n",
    "There are various shortcomings in using a training test split for training an algorithm. One main disadvantage is that while a simple train-test split is cheap, it is unreliable for future performance, especially when the dataset used is very small. \"There will also not be enough data in the test set to effectively evaluate the model performance. The estimated performance could be overly optimistic (good) or overly pessimistic (bad).\" (Brownlee, 2020), meaning that our model may be more unpredictable when testing the data and, as a result, may not be fully accurate. \n",
    "    \n",
    "A better way to test the data would be to use Cross Validation (CV). There are two types that can be used, Leave One Out Cross Validation or k-fold Cross Validation. K-fold separates the data into k number of train-test splits. This means that for each iteration different values in the dataset will be used for each test set. Therefore we obtain a higher proportion of testing data overall, as more samples are used in testing, meaning a more reliable future performance. LOOCV takes one sample from the dataset for testing, trains the rest of the data and then repeats this for all the samples in the dataset. As every sample is being tested once in LOOCV it means we obtain the highest amount of testing data possible, which thus makes it the most reliable for future performance. A drawback in using CV for k-fold, is that it can get computationally expensive if k is large. In using LOOCV, every sample is being used for CV and so large datasets will also be computationally expensive. \n",
    "    \n",
    "We could also, if we were to use a simple train-test split, introduce a validation set of say 10% of the data. This could be used to help prevent overfitting our model and is done during training to see how well the model is perfoming on unseen data. As the data is trained, both the training and validation error will decrease. However, if the validation error starts to increase, then the model starts to be overfitted and the programmer can choose to stop training the model. This is called early stopping.\n",
    "\n",
    "References:\n",
    "\n",
    "Brownlee, J. (2020) Train-test split for Evaluating Machine Learning Algorithms, MachineLearningMastery.com. Available from https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/ [accessed 17 December 2022]. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
